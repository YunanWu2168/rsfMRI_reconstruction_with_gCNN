{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1573,
     "status": "ok",
     "timestamp": 1587783935152,
     "user": {
      "displayName": "Pierre Besson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjsjJTjx005_2UQYExcZ9CVu8Atj0_UN1VGKPsHgg=s64",
      "userId": "17266492722066909081"
     },
     "user_tz": 300
    },
    "id": "S7ZbxgsvjCiw",
    "outputId": "f24e8d03-1713-4750-ca38-b78c82d5d258"
   },
   "outputs": [],
   "source": [
    "# %tensorflow_version 2.x\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)\n",
    "# GD = \"/content/drive/My Drive/DeepLearning/\"\n",
    "# import sys\n",
    "# sys.path.append(GD + 'code/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.sparse import block_diag as sparse_block_diag\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from utils.labels_preprocessing import load_labels\n",
    "from utils.load_synced_inputs import load_inputs, load_fmri\n",
    "from utils.load_graphs import load_graph\n",
    "from cnn_graph.resnetgraph import cgcnn\n",
    "from cnn_graph import graph\n",
    "import tensorflow.keras.backend as K\n",
    "from new_utils import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fmri preprocess\n",
    "\n",
    "fmri_subcortical_dir = 'path for the Subcortical_fmri dataset'\n",
    "# fmri_subcortical_dir = 'C:/Users/YuNan/Downloads/auto_encoder_try/data/Subcortical_fmri'\n",
    "assert len(os.listdir(fmri_subcortical_dir)) == len(set(os.listdir(fmri_subcortical_dir)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<14848x14848 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 103768 stored elements in Compressed Sparse Row format>, <7424x7424 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 51800 stored elements in Compressed Sparse Row format>, <3712x3712 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 25816 stored elements in Compressed Sparse Row format>, <1856x1856 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 12824 stored elements in Compressed Sparse Row format>, <928x928 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 6328 stored elements in Compressed Sparse Row format>, <464x464 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 3080 stored elements in Compressed Sparse Row format>, <232x232 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 1456 stored elements in Compressed Sparse Row format>, <116x116 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 648 stored elements in Compressed Sparse Row format>]\n"
     ]
    }
   ],
   "source": [
    "home = f\"{os.getcwd()}/..\"\n",
    "\n",
    "graphs_dir = \"../Graphs/\"            # directory containing necessary graphs\n",
    "# print(graphs_dir)\n",
    "# cortical_dir = f\"{home}/Cortical\"         # cortical node data for each subject\n",
    "# subcortical_dir = f\"{home}/SubCortical\"   # subcortical node data for each subject\n",
    "# cortical_dir = 'C:/Users/YuNan/Downloads/auto_encoder_try/data/Cortex'\n",
    "subcortical_dir = '../Subcortical'\n",
    "\n",
    "# for scan in os.listdir(cortical_dir):\n",
    "#     print (f\"{scan[:-11]}_Subcortical\")\n",
    "#     assert os.path.isfile(f\"{subcortical_dir}/{scan[:-11]}_Subcortical.mat\")\n",
    "\n",
    "assert len(os.listdir(subcortical_dir)) == len(set(os.listdir(subcortical_dir)))\n",
    "\n",
    "# L = {'cortical': load_graph(graphs_folder=graphs_dir, prefix='M_w')}\n",
    "# # print(L)\n",
    "# L['cortical'] = [graph.laplacian(A) for A in L['cortical']]\n",
    "# # print(L['cortical'])\n",
    "\n",
    "L = {'subcortical' : load_graph(graphs_folder=graphs_dir, prefix='M_sc_w')}\n",
    "L['subcortical'] = [graph.laplacian(A) for A in L['subcortical']]\n",
    "print(L['subcortical'])\n",
    "\n",
    "# # Block Diagonalize the Cortical & Subcortical Laplacians at Each Level of Coarsening\n",
    "new_L = []\n",
    "for A in zip(L[f\"subcortical\"]):\n",
    "    new_L.append( sparse_block_diag((A)) )\n",
    "L = new_L\n",
    "# print(L)\n",
    "del new_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<14848x14848 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 103768 stored elements in Compressed Sparse Row format>,\n",
       " <7424x7424 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 51800 stored elements in Compressed Sparse Row format>,\n",
       " <3712x3712 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 25816 stored elements in Compressed Sparse Row format>,\n",
       " <1856x1856 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 12824 stored elements in Compressed Sparse Row format>,\n",
       " <928x928 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 6328 stored elements in Compressed Sparse Row format>,\n",
       " <464x464 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 3080 stored elements in Compressed Sparse Row format>,\n",
       " <232x232 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 1456 stored elements in Compressed Sparse Row format>,\n",
       " <116x116 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 648 stored elements in Compressed Sparse Row format>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000102_20252_2_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000710_20252_2_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000742_20252_2_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001275_20252_2_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001693_20252_2_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>1030712_20252_2_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>1030841_20252_2_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>1030960_20252_2_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>1031057_20252_2_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>1031135_20252_2_0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>195 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Subject\n",
       "0    1000102_20252_2_0\n",
       "1    1000710_20252_2_0\n",
       "2    1000742_20252_2_0\n",
       "3    1001275_20252_2_0\n",
       "4    1001693_20252_2_0\n",
       "..                 ...\n",
       "190  1030712_20252_2_0\n",
       "191  1030841_20252_2_0\n",
       "192  1030960_20252_2_0\n",
       "193  1031057_20252_2_0\n",
       "194  1031135_20252_2_0\n",
       "\n",
       "[195 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dataset \n",
    "import pandas as pd\n",
    "Path = '../data.csv'   # table for all the subjects\n",
    "df = pd.read_csv(Path)\n",
    "filenames = df['Subject'].astype('str').to_numpy()\n",
    "# print(filenames)\n",
    "filenames2 = (filenames + '_Subcortical' +'.mat').tolist()\n",
    "# print(filenames2)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subcortical\t(Samples, Nodes, Feat.):\t(195, 14848, 490)\n"
     ]
    }
   ],
   "source": [
    "# fmri \n",
    "graph_type = {'subcortical':{'path':fmri_subcortical_dir, \n",
    "                             'halves':False}\n",
    "}\n",
    "\n",
    "\n",
    "X = load_fmri(graph_type=graph_type, filenames = filenames2)\n",
    "\n",
    "sub_graphs = list(graph_type.keys())\n",
    "assert sub_graphs == ['subcortical']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try\n",
    "X_try = {}\n",
    "X_try['subcortical'] = X['subcortical'][:100,:,10:]\n",
    "del X\n",
    "# X_try['subcortical'] = X['subcortical'][:,:,0:200]\n",
    "X_try['subcortical'].shape\n",
    "X = X_try\n",
    "del X_try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 14848, 480)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['subcortical'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.019178856 40.186802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100, 14848, 480)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fmri block\n",
    "from scipy.linalg import block_diag\n",
    "from sklearn.utils import resample\n",
    "\n",
    "all_nodes = np.concatenate([X[sub].flatten() for sub in sub_graphs])\n",
    "# X_max = np.max(all_nodes)\n",
    "# X_min = np.min(all_nodes)\n",
    "X_mean = np.mean(all_nodes)\n",
    "X_std = np.std(all_nodes)\n",
    "\n",
    "print(X_mean, X_std)\n",
    "\n",
    "for sub in sub_graphs:\n",
    "    X[sub] = (X[sub] - X_mean) / X_std\n",
    "#     X[sub] *= 2\n",
    "#     X[sub] -= 1\n",
    "del all_nodes   \n",
    "all_nodes = np.concatenate([X[sub].flatten() for sub in sub_graphs])\n",
    "del all_nodes\n",
    "\n",
    "new_X = []\n",
    "for A in zip(X[f\"subcortical\"]):\n",
    "    new_X.append(np.concatenate((A), axis = 0))\n",
    "    \n",
    "X = np.concatenate([np.expand_dims(i, axis=0) for i in new_X], axis=0).astype('float32')\n",
    "X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split with time point\n",
    "new_X = []\n",
    "# bb.T.reshape()\n",
    "for i in range(X.shape[0]):\n",
    "    new_X.append(X[i,:,:].T)\n",
    "new_XX = np.concatenate([np.expand_dims(i, axis=0) for i in new_X], axis=0).astype('float32')\n",
    "X_re = np.reshape(new_XX, (X.shape[0]* X.shape[2], X.shape[1], 1))\n",
    "X_re.shape\n",
    "del X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48000, 14848, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# fig = X_re[100, :, :]\n",
    "# plt.plot(fig)\n",
    "X_re.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 14848)\n",
      "False\n",
      "False\n",
      "False\n",
      "16.902107 -20.660309\n"
     ]
    }
   ],
   "source": [
    "tryy = np.squeeze(X_re, axis=-1)\n",
    "print(tryy.shape)\n",
    "print(np.any(np.isnan(tryy)))\n",
    "print(np.any(np.isposinf(tryy)))\n",
    "print(np.any(np.isneginf(tryy)))\n",
    "print(np.max(tryy), np.min(tryy))\n",
    "del tryy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "goQRb1A3jY5y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38400, 14848, 1) (9600, 14848, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 16\n",
    "epochs = 50\n",
    "n_filter = 32\n",
    "\n",
    "# aug_training = dict()\n",
    "# aug_training[\"max_rotation\"] = 40\n",
    "# aug_training[\"max_zoom\"] = 0.05\n",
    "# aug_training[\"max_noise\"] = 0.000\n",
    "# aug_training[\"max_shift\"] = 0.001\n",
    "# aug_training[\"prob_augment\"] = 1.\n",
    "\n",
    "# aug_testing = dict()\n",
    "# aug_testing[\"max_rotation\"] = 20\n",
    "# aug_testing[\"max_zoom\"] = 0.025\n",
    "# aug_testing[\"max_noise\"] = 0.000\n",
    "# aug_testing[\"max_shift\"] = 0.0005\n",
    "# aug_testing[\"prob_augment\"] = 1.\n",
    "\n",
    "X_train, X_test = X_re[:80*480,:,:], X_re[80*480:,:,:]\n",
    "# X_ttest = X_re[39000:,:,:]\n",
    "del X_re\n",
    "print(X_train.shape, X_test.shape)\n",
    "train_generator = utils.DataGenerator(X_train, \n",
    "                                      batch_size = batch_size, \n",
    "                                      shuffle=True, \n",
    "                                      force_balance=False, \n",
    "                                      do_augmentation = False, \n",
    "#                                       tta=10, \n",
    "#                                       aug_params=aug_training,\n",
    "                                      auto_encoder=True)\n",
    "test_generator  = utils.DataGenerator(X_test,  \n",
    "                                      batch_size = batch_size, \n",
    "                                      shuffle=False, \n",
    "                                      force_balance=False, \n",
    "                                      do_augmentation = False, \n",
    "#                                       tta=10, \n",
    "#                                       aug_params=aug_testing,\n",
    "                                      auto_encoder=True)\n",
    "\n",
    "# ttest_generator  = utils.DataGenerator(X_ttest,  \n",
    "#                                       batch_size = 1, \n",
    "#                                       shuffle=False, \n",
    "#                                       force_balance=False, \n",
    "#                                       do_augmentation = False, \n",
    "# #                                       tta=10, \n",
    "# #                                       aug_params=aug_testing,\n",
    "#                                       auto_encoder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_generator))\n",
    "len(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1ef53ba3d48>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = X_train[0, :, :]\n",
    "plt.plot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9600, 14848, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C4o5Wr64kf7C"
   },
   "source": [
    "## Architecture of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a4qyWDmujQnE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 14848, 1)\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 14848, 1)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cheb_layer (ChebLayer)          (None, 14848, 32)    128         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 14848, 32)    128         cheb_layer[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 14848, 32)    0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "cheb_layer_1 (ChebLayer)        (None, 14848, 32)    3072        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 14848, 32)    128         cheb_layer_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 14848, 32)    0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "cheb_layer_2 (ChebLayer)        (None, 14848, 32)    3104        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 14848, 32)    128         cheb_layer_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 14848, 32)    0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "cheb_layer_3 (ChebLayer)        (None, 14848, 32)    3072        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 14848, 32)    128         cheb_layer_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 14848, 32)    0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 7424, 32)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cheb_layer_4 (ChebLayer)        (None, 7424, 32)     3104        max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 7424, 32)     128         cheb_layer_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 7424, 32)     0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "cheb_layer_5 (ChebLayer)        (None, 7424, 32)     3072        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 7424, 32)     128         cheb_layer_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 7424, 32)     0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "cheb_layer_6 (ChebLayer)        (None, 7424, 32)     3104        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 7424, 32)     128         cheb_layer_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 7424, 32)     0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "cheb_layer_7 (ChebLayer)        (None, 7424, 32)     3072        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 7424, 32)     128         cheb_layer_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 7424, 32)     0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 3712, 32)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cheb_layer_8 (ChebLayer)        (None, 3712, 32)     3104        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 3712, 32)     128         cheb_layer_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 3712, 32)     0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "cheb_layer_9 (ChebLayer)        (None, 3712, 32)     3072        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 3712, 32)     128         cheb_layer_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 3712, 32)     0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "cheb_layer_10 (ChebLayer)       (None, 3712, 32)     3104        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 3712, 32)     128         cheb_layer_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 3712, 32)     0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "cheb_layer_11 (ChebLayer)       (None, 3712, 32)     3072        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 3712, 32)     128         cheb_layer_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 3712, 32)     0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 1856, 32)     0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cheb_layer_12 (ChebLayer)       (None, 1856, 32)     3104        max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 1856, 32)     128         cheb_layer_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 1856, 32)     0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "cheb_layer_13 (ChebLayer)       (None, 1856, 32)     3072        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 1856, 32)     128         cheb_layer_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 1856, 32)     0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "cheb_layer_14 (ChebLayer)       (None, 1856, 32)     3104        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 1856, 32)     128         cheb_layer_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1856, 32)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "cheb_layer_15 (ChebLayer)       (None, 1856, 32)     3072        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 1856, 32)     128         cheb_layer_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 1856, 32)     0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 928, 32)      0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cheb_layer_16 (ChebLayer)       (None, 928, 32)      3104        max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 928, 32)      128         cheb_layer_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 928, 32)      0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "cheb_layer_17 (ChebLayer)       (None, 928, 32)      3072        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 928, 32)      128         cheb_layer_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 928, 32)      0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "cheb_layer_18 (ChebLayer)       (None, 928, 32)      3104        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 928, 32)      128         cheb_layer_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 928, 32)      0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "cheb_layer_19 (ChebLayer)       (None, 928, 32)      3072        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 928, 32)      128         cheb_layer_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 928, 32)      0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 464, 32)      0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cheb_layer_20 (ChebLayer)       (None, 464, 32)      3104        max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 464, 32)      128         cheb_layer_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 464, 32)      0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "cheb_layer_21 (ChebLayer)       (None, 464, 32)      3072        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 464, 32)      128         cheb_layer_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 464, 32)      0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "cheb_layer_22 (ChebLayer)       (None, 464, 32)      3104        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 464, 32)      128         cheb_layer_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 464, 32)      0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "cheb_layer_23 (ChebLayer)       (None, 464, 32)      3072        activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 464, 32)      128         cheb_layer_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 464, 32)      0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 232, 32)      0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cheb_layer_24 (ChebLayer)       (None, 232, 32)      3104        max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 232, 32)      128         cheb_layer_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 232, 32)      0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "cheb_layer_25 (ChebLayer)       (None, 232, 32)      3072        activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 232, 32)      128         cheb_layer_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 232, 32)      0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "cheb_layer_26 (ChebLayer)       (None, 232, 32)      3104        activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 232, 32)      128         cheb_layer_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 232, 32)      0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "cheb_layer_27 (ChebLayer)       (None, 232, 32)      3072        activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 232, 32)      128         cheb_layer_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 232, 32)      0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 116, 32)      0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 3712)         0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 256)          950528      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 256)          950528      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "encoder_output (Lambda)         (None, 256)          0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,988,128\n",
      "Trainable params: 1,986,336\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 256)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3712)              953984    \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 116, 32)           0         \n",
      "_________________________________________________________________\n",
      "cheb_layer_28 (ChebLayer)    (None, 116, 32)           3072      \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 116, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 116, 32)           0         \n",
      "_________________________________________________________________\n",
      "cheb_layer_29 (ChebLayer)    (None, 116, 32)           3072      \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 116, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 116, 32)           0         \n",
      "_________________________________________________________________\n",
      "cheb_layer_30 (ChebLayer)    (None, 116, 32)           3072      \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 116, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 116, 32)           0         \n",
      "_________________________________________________________________\n",
      "cheb_layer_31 (ChebLayer)    (None, 116, 32)           3072      \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 116, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 116, 32)           0         \n",
      "_________________________________________________________________\n",
      "up_sampling1d (UpSampling1D) (None, 232, 32)           0         \n",
      "_________________________________________________________________\n",
      "cheb_layer_32 (ChebLayer)    (None, 232, 32)           3072      \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 232, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 232, 32)           0         \n",
      "_________________________________________________________________\n",
      "cheb_layer_33 (ChebLayer)    (None, 232, 32)           3072      \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 232, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 232, 32)           0         \n",
      "_________________________________________________________________\n",
      "cheb_layer_34 (ChebLayer)    (None, 232, 32)           3072      \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 232, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 232, 32)           0         \n",
      "_________________________________________________________________\n",
      "cheb_layer_35 (ChebLayer)    (None, 232, 32)           3072      \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 232, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 232, 32)           0         \n",
      "_________________________________________________________________\n",
      "up_sampling1d_1 (UpSampling1 (None, 464, 32)           0         \n",
      "_________________________________________________________________\n",
      "cheb_layer_36 (ChebLayer)    (None, 464, 32)           3072      \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 464, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 464, 32)           0         \n",
      "_________________________________________________________________\n",
      "cheb_layer_37 (ChebLayer)    (None, 464, 32)           3072      \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 464, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 464, 32)           0         \n",
      "_________________________________________________________________\n",
      "cheb_layer_38 (ChebLayer)    (None, 464, 32)           3072      \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 464, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 464, 32)           0         \n",
      "_________________________________________________________________\n",
      "cheb_layer_39 (ChebLayer)    (None, 464, 32)           3072      \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 464, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 464, 32)           0         \n",
      "_________________________________________________________________\n",
      "up_sampling1d_2 (UpSampling1 (None, 928, 32)           0         \n",
      "_________________________________________________________________\n",
      "cheb_layer_40 (ChebLayer)    (None, 928, 32)           3072      \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 928, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 928, 32)           0         \n",
      "_________________________________________________________________\n",
      "cheb_layer_41 (ChebLayer)    (None, 928, 32)           3072      \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 928, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 928, 32)           0         \n",
      "_________________________________________________________________\n",
      "cheb_layer_42 (ChebLayer)    (None, 928, 32)           3072      \n",
      "_________________________________________________________________\n",
      "batch_normalization_42 (Batc (None, 928, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 928, 32)           0         \n",
      "_________________________________________________________________\n",
      "cheb_layer_43 (ChebLayer)    (None, 928, 32)           3072      \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 928, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 928, 32)           0         \n",
      "_________________________________________________________________\n",
      "up_sampling1d_3 (UpSampling1 (None, 1856, 32)          0         \n",
      "_________________________________________________________________\n",
      "cheb_layer_44 (ChebLayer)    (None, 1856, 32)          3072      \n",
      "_________________________________________________________________\n",
      "batch_normalization_44 (Batc (None, 1856, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 1856, 32)          0         \n",
      "_________________________________________________________________\n",
      "cheb_layer_45 (ChebLayer)    (None, 1856, 32)          3072      \n",
      "_________________________________________________________________\n",
      "batch_normalization_45 (Batc (None, 1856, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 1856, 32)          0         \n",
      "_________________________________________________________________\n",
      "cheb_layer_46 (ChebLayer)    (None, 1856, 32)          3072      \n",
      "_________________________________________________________________\n",
      "batch_normalization_46 (Batc (None, 1856, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 1856, 32)          0         \n",
      "_________________________________________________________________\n",
      "cheb_layer_47 (ChebLayer)    (None, 1856, 32)          3072      \n",
      "_________________________________________________________________\n",
      "batch_normalization_47 (Batc (None, 1856, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 1856, 32)          0         \n",
      "_________________________________________________________________\n",
      "up_sampling1d_4 (UpSampling1 (None, 3712, 32)          0         \n",
      "_________________________________________________________________\n",
      "cheb_layer_48 (ChebLayer)    (None, 3712, 32)          3072      \n",
      "_________________________________________________________________\n",
      "batch_normalization_48 (Batc (None, 3712, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 3712, 32)          0         \n",
      "_________________________________________________________________\n",
      "cheb_layer_49 (ChebLayer)    (None, 3712, 32)          3072      \n",
      "_________________________________________________________________\n",
      "batch_normalization_49 (Batc (None, 3712, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 3712, 32)          0         \n",
      "_________________________________________________________________\n",
      "cheb_layer_50 (ChebLayer)    (None, 3712, 32)          3072      \n",
      "_________________________________________________________________\n",
      "batch_normalization_50 (Batc (None, 3712, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 3712, 32)          0         \n",
      "_________________________________________________________________\n",
      "cheb_layer_51 (ChebLayer)    (None, 3712, 32)          3072      \n",
      "_________________________________________________________________\n",
      "batch_normalization_51 (Batc (None, 3712, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 3712, 32)          0         \n",
      "_________________________________________________________________\n",
      "up_sampling1d_5 (UpSampling1 (None, 7424, 32)          0         \n",
      "_________________________________________________________________\n",
      "cheb_layer_52 (ChebLayer)    (None, 7424, 32)          3072      \n",
      "_________________________________________________________________\n",
      "batch_normalization_52 (Batc (None, 7424, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 7424, 32)          0         \n",
      "_________________________________________________________________\n",
      "cheb_layer_53 (ChebLayer)    (None, 7424, 32)          3072      \n",
      "_________________________________________________________________\n",
      "batch_normalization_53 (Batc (None, 7424, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 7424, 32)          0         \n",
      "_________________________________________________________________\n",
      "cheb_layer_54 (ChebLayer)    (None, 7424, 32)          3072      \n",
      "_________________________________________________________________\n",
      "batch_normalization_54 (Batc (None, 7424, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 7424, 32)          0         \n",
      "_________________________________________________________________\n",
      "cheb_layer_55 (ChebLayer)    (None, 7424, 32)          3072      \n",
      "_________________________________________________________________\n",
      "batch_normalization_55 (Batc (None, 7424, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 7424, 32)          0         \n",
      "_________________________________________________________________\n",
      "up_sampling1d_6 (UpSampling1 (None, 14848, 32)         0         \n",
      "_________________________________________________________________\n",
      "cheb_layer_56 (ChebLayer)    (None, 14848, 1)          96        \n",
      "=================================================================\n",
      "Total params: 1,043,680\n",
      "Trainable params: 1,041,888\n",
      "Non-trainable params: 1,792\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 14848, 1)]        0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              [(None, 256), (None, 256) 1988128   \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 14848, 1)          1043680   \n",
      "=================================================================\n",
      "Total params: 3,031,808\n",
      "Trainable params: 3,028,224\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import mse\n",
    "from tensorflow.keras import layers\n",
    "Depth = 7\n",
    "latent_dim = 256\n",
    "# Input\n",
    "encoder_inputs = keras.Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "print(encoder_inputs.shape)\n",
    "\n",
    "# Encoder ResNet Blocks\n",
    "for B in range(Depth):\n",
    "    for F in range(2):\n",
    "#         print(B)\n",
    "#         print(L[B].shape)\n",
    "        if B == 0 and F == 0:\n",
    "            x1 = utils.ChebLayer(L[B],3,n_filter,use_bias=True,kernel_regularizer=keras.regularizers.l2(5e-5))(encoder_inputs)\n",
    "        else:\n",
    "            x1 = utils.ChebLayer(L[B],3,n_filter,use_bias=True,kernel_regularizer=keras.regularizers.l2(5e-5))(x1)\n",
    "        x1 = keras.layers.BatchNormalization()(x1)\n",
    "        x1 = keras.layers.Activation('elu')(x1)\n",
    "        #x1 = keras.layers.MaxPooling1D(2)(x1)\n",
    "        x1 = utils.ChebLayer(L[B],3,n_filter,use_bias=False,kernel_regularizer=keras.regularizers.l2(5e-5))(x1)\n",
    "        x1 = keras.layers.BatchNormalization()(x1)\n",
    "        x1 = keras.layers.Activation('elu')(x1)\n",
    "    x1 = keras.layers.MaxPooling1D(2)(x1)\n",
    "#     print(x1.shape)\n",
    "\n",
    "x = layers.Flatten()(x1)\n",
    "# x = layers.Dense(128, activation=\"relu\")(x)\n",
    "\n",
    "z_mean = layers.Dense(latent_dim, name = 'z_mean')(x)\n",
    "z_log_var = layers.Dense(latent_dim, name = 'z_log_var')(x)\n",
    "\n",
    "# Defining a function for sampling\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=K.shape(z_mean), mean=0., stddev=1.) \n",
    "    return z_mean + K.exp(z_log_var/2)*epsilon \n",
    "z = layers.Lambda(sampling, name='encoder_output')([z_mean, z_log_var])\n",
    "\n",
    "\n",
    "# z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "# z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "# z = Sampling()([z_mean, z_log_var])\n",
    "\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()\n",
    "\n",
    "# Decoder ResNet Blocks\n",
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(3712)(latent_inputs)\n",
    "x = layers.Reshape((116, 32))(x)\n",
    "# print(x.shape)\n",
    "for B in range(Depth):\n",
    "    for F in range(2):\n",
    "        #print(L[Depth - B].shape)\n",
    "        if B == 0 and F == 0:\n",
    "            x1 = utils.ChebLayer(L[Depth - B],3,n_filter,use_bias=False,kernel_regularizer=keras.regularizers.l2(5e-5))(x)\n",
    "        else:\n",
    "            x1 = utils.ChebLayer(L[Depth - B],3,n_filter,use_bias=False,kernel_regularizer=keras.regularizers.l2(5e-5))(x1)\n",
    "        x1 = keras.layers.BatchNormalization()(x1)\n",
    "        x1 = keras.layers.Activation('elu')(x1)\n",
    "        #x1 = keras.layers.UpSampling1D(size=2)(x1)\n",
    "        x1 = utils.ChebLayer(L[Depth - B],3,n_filter,use_bias=False,kernel_regularizer=keras.regularizers.l2(5e-5))(x1)\n",
    "        x1 = keras.layers.BatchNormalization()(x1)\n",
    "        x1 = keras.layers.Activation('elu')(x1)\n",
    "    x1 = keras.layers.UpSampling1D(size=2)(x1)\n",
    "decoder_outputs = utils.ChebLayer(L[0],3,X_train.shape[2],use_bias=False,kernel_regularizer=keras.regularizers.l2(5e-5))(x1)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()\n",
    "\n",
    "# Instantiate VAE\n",
    "vae_outputs = decoder(encoder(encoder_inputs)[2])\n",
    "vae         = Model(encoder_inputs, vae_outputs, name='vae')\n",
    "vae.summary()\n",
    "\n",
    "\n",
    "# # opt_m = tf.keras.optimizers.Adam(lr=0.0001)\n",
    "# opt_m = tf.keras.optimizers.RMSprop(learning_rate=0.0005)\n",
    "# # vae.compile(opt_m, loss = total_loss, metrics = [r_loss, kl_loss])\n",
    "# vae.compile(opt_m, loss = 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0002\n",
    "LOSS_FACTOR = 1000\n",
    "\n",
    "def r_loss(y_true, y_pred):\n",
    "    return K.mean(K.square(y_true - y_pred), axis = [1,2])\n",
    "\n",
    "def kl_loss(y_true, y_pred):\n",
    "    kl_loss =  -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis = 1)\n",
    "    return kl_loss\n",
    "\n",
    "def total_loss(y_true, y_pred):\n",
    "    return LOSS_FACTOR*r_loss(y_true, y_pred) + kl_loss(y_true, y_pred)\n",
    "  \n",
    "adam_optimizer = keras.optimizers.Adam(lr = LEARNING_RATE)\n",
    "\n",
    "vae.compile(optimizer=adam_optimizer, loss = total_loss, metrics = [r_loss, kl_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "es = EarlyStopping(monitor='val_loss', \n",
    "                   verbose=1, \n",
    "                   patience=8, \n",
    "                   min_delta=0.0001, \n",
    "                   mode='min')\n",
    "mc = ModelCheckpoint('.../models/best_cortex_try3.hdf5', \n",
    "                     monitor='val_loss', \n",
    "                     verbose=1, \n",
    "                     save_best_only=True, \n",
    "                     mode='min')\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                        mode='min',\n",
    "                        factor=0.1,\n",
    "                        patience=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vMyX9D3buIIM",
    "outputId": "7cedd5f0-1c86-40b1-e9d1-8b76aaa86928"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "9599/9600 [============================>.] - ETA: 0s - loss: 498.5373 - r_loss: 0.9566 - kl_loss: 19.8463Epoch 1/50\n",
      "2400/9600 [======>.......................] - ETA: 19:21 - loss: 594.9173 - r_loss: 1.1645 - kl_loss: 12.2807\n",
      "Epoch 00001: val_loss improved from inf to 594.91726, saving model to C:/Users/YuNan/Downloads/auto_encoder_try/models/best_cortex_try3.hdf5\n",
      "9600/9600 [==============================] - 5313s 553ms/step - loss: 498.5302 - r_loss: 0.9566 - kl_loss: 19.8449 - val_loss: 594.9173 - val_r_loss: 1.1645 - val_kl_loss: 12.2807\n",
      "Epoch 2/50\n",
      "9599/9600 [============================>.] - ETA: 0s - loss: 465.7392 - r_loss: 0.9215 - kl_loss: 4.5782Epoch 1/50\n",
      "2400/9600 [======>.......................] - ETA: 18:51 - loss: 655.1228 - r_loss: 1.2683 - kl_loss: 20.5155\n",
      "Epoch 00002: val_loss did not improve from 594.91726\n",
      "9600/9600 [==============================] - 5139s 535ms/step - loss: 465.7470 - r_loss: 0.9215 - kl_loss: 4.5786 - val_loss: 655.1228 - val_r_loss: 1.2683 - val_kl_loss: 20.5155\n",
      "Epoch 3/50\n",
      "9599/9600 [============================>.] - ETA: 0s - loss: 437.7258 - r_loss: 0.8524 - kl_loss: 11.0656Epoch 1/50\n",
      "2400/9600 [======>.......................] - ETA: 18:51 - loss: 475.8519 - r_loss: 0.9218 - kl_loss: 14.4772\n",
      "Epoch 00003: val_loss improved from 594.91726 to 475.85195, saving model to C:/Users/YuNan/Downloads/auto_encoder_try/models/best_cortex_try3.hdf5\n",
      "9600/9600 [==============================] - 5159s 537ms/step - loss: 437.7393 - r_loss: 0.8524 - kl_loss: 11.0655 - val_loss: 475.8519 - val_r_loss: 0.9218 - val_kl_loss: 14.4772\n",
      "Epoch 4/50\n",
      "9599/9600 [============================>.] - ETA: 0s - loss: 426.0339 - r_loss: 0.8265 - kl_loss: 12.2859Epoch 1/50\n",
      "2400/9600 [======>.......................] - ETA: 18:50 - loss: 441.9955 - r_loss: 0.8568 - kl_loss: 13.0776\n",
      "Epoch 00004: val_loss improved from 475.85195 to 441.99546, saving model to C:/Users/YuNan/Downloads/auto_encoder_try/models/best_cortex_try3.hdf5\n",
      "9600/9600 [==============================] - 5137s 535ms/step - loss: 426.0660 - r_loss: 0.8265 - kl_loss: 12.2861 - val_loss: 441.9955 - val_r_loss: 0.8568 - val_kl_loss: 13.0776\n",
      "Epoch 5/50\n",
      "9599/9600 [============================>.] - ETA: 0s - loss: 418.6943 - r_loss: 0.8106 - kl_loss: 12.8435Epoch 1/50\n",
      "2400/9600 [======>.......................] - ETA: 18:50 - loss: 442.1124 - r_loss: 0.8509 - kl_loss: 16.0665\n",
      "Epoch 00005: val_loss did not improve from 441.99546\n",
      "9600/9600 [==============================] - 5133s 535ms/step - loss: 418.6964 - r_loss: 0.8106 - kl_loss: 12.8434 - val_loss: 442.1124 - val_r_loss: 0.8509 - val_kl_loss: 16.0665\n",
      "Epoch 6/50\n",
      "9599/9600 [============================>.] - ETA: 0s - loss: 412.1547 - r_loss: 0.7964 - kl_loss: 13.2737Epoch 1/50\n",
      "2400/9600 [======>.......................] - ETA: 18:50 - loss: 445.2061 - r_loss: 0.8637 - kl_loss: 12.6815\n",
      "Epoch 00006: val_loss did not improve from 441.99546\n",
      "9600/9600 [==============================] - 5133s 535ms/step - loss: 412.1416 - r_loss: 0.7964 - kl_loss: 13.2737 - val_loss: 445.2061 - val_r_loss: 0.8637 - val_kl_loss: 12.6815\n",
      "Epoch 7/50\n",
      "9599/9600 [============================>.] - ETA: 0s - loss: 407.2262 - r_loss: 0.7860 - kl_loss: 13.5225Epoch 1/50\n",
      "2400/9600 [======>.......................] - ETA: 18:50 - loss: 438.1915 - r_loss: 0.8481 - kl_loss: 13.4046\n",
      "Epoch 00007: val_loss improved from 441.99546 to 438.19150, saving model to C:/Users/YuNan/Downloads/auto_encoder_try/models/best_cortex_try3.hdf5\n",
      "9600/9600 [==============================] - 5133s 535ms/step - loss: 407.2300 - r_loss: 0.7860 - kl_loss: 13.5226 - val_loss: 438.1915 - val_r_loss: 0.8481 - val_kl_loss: 13.4046\n",
      "Epoch 8/50\n",
      "9599/9600 [============================>.] - ETA: 0s - loss: 403.8161 - r_loss: 0.7784 - kl_loss: 13.8239Epoch 1/50\n",
      "2400/9600 [======>.......................] - ETA: 18:51 - loss: 438.9866 - r_loss: 0.8450 - kl_loss: 15.7087\n",
      "Epoch 00008: val_loss did not improve from 438.19150\n",
      "9600/9600 [==============================] - 5132s 535ms/step - loss: 403.8129 - r_loss: 0.7784 - kl_loss: 13.8238 - val_loss: 438.9866 - val_r_loss: 0.8450 - val_kl_loss: 15.7087\n",
      "Epoch 9/50\n",
      "9599/9600 [============================>.] - ETA: 0s - loss: 401.7435 - r_loss: 0.7737 - kl_loss: 14.0681Epoch 1/50\n",
      "2400/9600 [======>.......................] - ETA: 18:51 - loss: 434.2476 - r_loss: 0.8321 - kl_loss: 17.3662\n",
      "Epoch 00009: val_loss improved from 438.19150 to 434.24760, saving model to C:/Users/YuNan/Downloads/auto_encoder_try/models/best_cortex_try3.hdf5\n",
      "9600/9600 [==============================] - 5137s 535ms/step - loss: 401.7441 - r_loss: 0.7737 - kl_loss: 14.0682 - val_loss: 434.2476 - val_r_loss: 0.8321 - val_kl_loss: 17.3662\n",
      "Epoch 10/50\n",
      "9599/9600 [============================>.] - ETA: 0s - loss: 400.0410 - r_loss: 0.7695 - kl_loss: 14.3986Epoch 1/50\n",
      "2400/9600 [======>.......................] - ETA: 18:52 - loss: 434.8651 - r_loss: 0.8324 - kl_loss: 17.7866\n",
      "Epoch 00010: val_loss did not improve from 434.24760\n",
      "9600/9600 [==============================] - 5131s 534ms/step - loss: 400.0402 - r_loss: 0.7695 - kl_loss: 14.3986 - val_loss: 434.8651 - val_r_loss: 0.8324 - val_kl_loss: 17.7866\n",
      "Epoch 11/50\n",
      "9599/9600 [============================>.] - ETA: 0s - loss: 398.3500 - r_loss: 0.7654 - kl_loss: 14.7447Epoch 1/50\n",
      "2400/9600 [======>.......................] - ETA: 18:52 - loss: 429.2613 - r_loss: 0.8273 - kl_loss: 14.6943\n",
      "Epoch 00011: val_loss improved from 434.24760 to 429.26132, saving model to C:/Users/YuNan/Downloads/auto_encoder_try/models/best_cortex_try3.hdf5\n",
      "9600/9600 [==============================] - 5136s 535ms/step - loss: 398.3768 - r_loss: 0.7654 - kl_loss: 14.7447 - val_loss: 429.2613 - val_r_loss: 0.8273 - val_kl_loss: 14.6943\n",
      "Epoch 12/50\n",
      "9599/9600 [============================>.] - ETA: 0s - loss: 396.8891 - r_loss: 0.7618 - kl_loss: 15.0438Epoch 1/50\n",
      "2400/9600 [======>.......................] - ETA: 19:00 - loss: 449.1734 - r_loss: 0.8724 - kl_loss: 11.9985\n",
      "Epoch 00012: val_loss did not improve from 429.26132\n",
      "9600/9600 [==============================] - 5150s 536ms/step - loss: 396.9150 - r_loss: 0.7618 - kl_loss: 15.0438 - val_loss: 449.1734 - val_r_loss: 0.8724 - val_kl_loss: 11.9985\n",
      "Epoch 13/50\n",
      "9599/9600 [============================>.] - ETA: 0s - loss: 395.8468 - r_loss: 0.7592 - kl_loss: 15.2656Epoch 1/50\n",
      "2400/9600 [======>.......................] - ETA: 18:59 - loss: 431.8901 - r_loss: 0.8290 - kl_loss: 16.3940\n",
      "Epoch 00013: val_loss did not improve from 429.26132\n",
      "9600/9600 [==============================] - 5155s 537ms/step - loss: 395.8441 - r_loss: 0.7592 - kl_loss: 15.2657 - val_loss: 431.8901 - val_r_loss: 0.8290 - val_kl_loss: 16.3940\n",
      "Epoch 14/50\n",
      "9599/9600 [============================>.] - ETA: 0s - loss: 394.8916 - r_loss: 0.7569 - kl_loss: 15.4137Epoch 1/50\n",
      "2400/9600 [======>.......................] - ETA: 19:00 - loss: 429.6117 - r_loss: 0.8275 - kl_loss: 14.8234\n",
      "Epoch 00014: val_loss did not improve from 429.26132\n",
      "9600/9600 [==============================] - 5152s 537ms/step - loss: 394.8810 - r_loss: 0.7569 - kl_loss: 15.4137 - val_loss: 429.6117 - val_r_loss: 0.8275 - val_kl_loss: 14.8234\n",
      "Epoch 15/50\n",
      "9599/9600 [============================>.] - ETA: 0s - loss: 394.0180 - r_loss: 0.7548 - kl_loss: 15.5725Epoch 1/50\n",
      "2400/9600 [======>.......................] - ETA: 18:48 - loss: 428.3152 - r_loss: 0.8211 - kl_loss: 16.6907\n",
      "Epoch 00015: val_loss improved from 429.26132 to 428.31523, saving model to C:/Users/YuNan/Downloads/auto_encoder_try/models/best_cortex_try3.hdf5\n",
      "9600/9600 [==============================] - 5165s 538ms/step - loss: 394.0236 - r_loss: 0.7548 - kl_loss: 15.5726 - val_loss: 428.3152 - val_r_loss: 0.8211 - val_kl_loss: 16.6907\n",
      "Epoch 16/50\n",
      "9599/9600 [============================>.] - ETA: 0s - loss: 393.2196 - r_loss: 0.7530 - kl_loss: 15.6043Epoch 1/50\n",
      "2400/9600 [======>.......................] - ETA: 19:14 - loss: 427.2076 - r_loss: 0.8207 - kl_loss: 15.7521\n",
      "Epoch 00016: val_loss improved from 428.31523 to 427.20758, saving model to C:/Users/YuNan/Downloads/auto_encoder_try/models/best_cortex_try3.hdf5\n",
      "9600/9600 [==============================] - 5195s 541ms/step - loss: 393.2124 - r_loss: 0.7530 - kl_loss: 15.6042 - val_loss: 427.2076 - val_r_loss: 0.8207 - val_kl_loss: 15.7521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50\n",
      " 725/9600 [=>............................] - ETA: 1:15:03 - loss: 393.9477 - r_loss: 0.7540 - kl_loss: 15.8149"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "GPU sync failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-cbfd671733ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrlr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\YuNan\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mC:\\Users\\YuNan\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    602\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 604\u001b[1;33m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    605\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mC:\\Users\\YuNan\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m       \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\YuNan\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1121\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1123\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1125\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\YuNan\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3565\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3566\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3567\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3568\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3569\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32mC:\\Users\\YuNan\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1472\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1473\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1474\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1475\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1476\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: GPU sync failed"
     ]
    }
   ],
   "source": [
    "history = vae.fit(train_generator, validation_data=test_generator, epochs = epochs, use_multiprocessing=False, callbacks = [es, mc,rlr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot history: Categorical crossentropy & Accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'], label='Categorical crossentropy (training data)')\n",
    "plt.plot(history.history['val_loss'], label='Categorical crossentropy (validation data)')\n",
    "# plt.plot(history.history['acc'], label='Accuracy (training data)')\n",
    "# plt.plot(history.history['val_acc'], label='Accuracy (validation data)')\n",
    "plt.title('Model performance for 3D MNIST Keras Conv3D example')\n",
    "plt.ylabel('Loss value')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMdSPYkDYl1nXjPSEKUrOVK",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "GraphAutoEncoder.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
